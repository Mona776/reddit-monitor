"""
Redditç›‘æµ‹å·¥å…· - ä¸»å…¥å£
ç›‘æ§Redditå¸–å­ã€è¯„è®ºå’Œå…³é”®è¯æœç´¢ï¼Œç”¨Geminiåˆ†æç›¸å…³æ€§ï¼Œæ¨é€åˆ°é£ä¹¦

ä¼˜åŒ–ç‰ˆï¼šå¢é‡å¤„ç†æ¨¡å¼
- é¢„è¿‡æ»¤å‡å°‘AIè°ƒç”¨
- æ¯æ‰¹æˆåŠŸåç«‹å³ä¿å­˜å’Œå‘é€
- å³ä½¿åç»­æ‰¹æ¬¡å¤±è´¥ï¼Œå‰é¢çš„ç»“æœä¸ä¼šä¸¢å¤±
"""

import os
import sys
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.reddit_fetcher import fetch_all_new_posts, load_processed_posts, save_processed_posts
from src.gemini_analyzer import analyze_batch, BATCH_SIZE, REQUEST_DELAY
from src.prefilter import pre_filter, prioritize_by_relevance
from src.feishu_notifier import send_batch_to_feishu, send_summary_to_feishu


def count_by_type(items: list) -> dict:
    """ç»Ÿè®¡å„ç±»å‹å†…å®¹æ•°é‡"""
    counts = {'post': 0, 'comment': 0, 'search': 0}
    for item in items:
        t = item.get('type', 'post')
        counts[t] = counts.get(t, 0) + 1
    return counts


def chunk_list(items: list, chunk_size: int) -> list:
    """å°†åˆ—è¡¨åˆ†æˆå›ºå®šå¤§å°çš„å—"""
    return [items[i:i + chunk_size] for i in range(0, len(items), chunk_size)]


def main():
    """ä¸»å‡½æ•° - å¢é‡å¤„ç†æ¨¡å¼"""
    print("=" * 60)
    print(f"Redditç›‘æµ‹å·¥å…·å¯åŠ¨ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.environ.get('GEMINI_API_KEY'):
        print("[é”™è¯¯] è¯·è®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    if not os.environ.get('FEISHU_WEBHOOK_URL'):
        print("[é”™è¯¯] è¯·è®¾ç½® FEISHU_WEBHOOK_URL ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    # æ­¥éª¤1: è·å–æ–°å†…å®¹ï¼ˆå¸–å­ã€è¯„è®ºã€æœç´¢ç»“æœï¼‰
    print("\nğŸ“¡ æ­¥éª¤1: è·å–Redditæ–°å†…å®¹...")
    new_items = fetch_all_new_posts()
    
    if not new_items:
        print("\nâœ… æ²¡æœ‰æ–°å†…å®¹éœ€è¦å¤„ç†ï¼Œé€€å‡º")
        return
    
    # ç»Ÿè®¡è·å–åˆ°çš„å†…å®¹
    fetch_stats = count_by_type(new_items)
    print(f"\n[è·å–å®Œæˆ] å…± {len(new_items)} æ¡æ–°å†…å®¹")
    
    # æ­¥éª¤2: é¢„è¿‡æ»¤
    print("\nğŸ” æ­¥éª¤2: é¢„è¿‡æ»¤...")
    filtered_items = pre_filter(new_items)
    
    if not filtered_items:
        print("\nâœ… é¢„è¿‡æ»¤åæ— å†…å®¹éœ€è¦åˆ†æï¼Œé€€å‡º")
        return
    
    # æŒ‰ç›¸å…³æ€§æ’åºï¼Œè®©æ›´å¯èƒ½ç›¸å…³çš„å†…å®¹å…ˆè¢«å¤„ç†
    filtered_items = prioritize_by_relevance(filtered_items)
    
    # æ­¥éª¤3: å¢é‡å¼AIåˆ†æ
    print("\nğŸ¤– æ­¥éª¤3: AIåˆ†æç›¸å…³æ€§ï¼ˆå¢é‡æ¨¡å¼ï¼‰...")
    
    # åŠ è½½å·²å¤„ç†è®°å½•
    processed_ids = load_processed_posts()
    
    # åˆ†æ‰¹å¤„ç†
    batches = chunk_list(filtered_items, BATCH_SIZE)
    total_batches = len(batches)
    
    print(f"  å…± {len(filtered_items)} æ¡å¾…åˆ†æï¼Œåˆ† {total_batches} æ‰¹å¤„ç†")
    print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}ï¼Œæ‰¹æ¬¡é—´éš”: {REQUEST_DELAY}ç§’")
    print("-" * 50)
    
    # ç»Ÿè®¡
    total_relevant = 0
    total_sent = 0
    relevant_stats = {'post': 0, 'comment': 0, 'search': 0}
    
    for batch_idx, batch_items in enumerate(batches):
        batch_num = batch_idx + 1
        
        # åˆ†æå½“å‰æ‰¹æ¬¡
        results = analyze_batch(batch_items, batch_num)
        
        # å¤„ç†åˆ†æç»“æœ
        relevant_in_batch = []
        for result in results:
            if not isinstance(result, dict):
                continue
            
            idx = result.get('index')
            if idx is None or idx >= len(batch_items):
                continue
            
            if result.get('is_relevant', False):
                item = batch_items[idx].copy()
                item['analysis'] = {
                    'is_relevant': True,
                    'reason': result.get('reason', ''),
                    'reply_draft': result.get('reply_draft', '')
                }
                relevant_in_batch.append(item)
                
                # æ›´æ–°ç»Ÿè®¡
                content_type = item.get('type', 'post')
                relevant_stats[content_type] = relevant_stats.get(content_type, 0) + 1
        
        # ç«‹å³å‘é€é£ä¹¦é€šçŸ¥ï¼ˆå¦‚æœæœ‰ç›¸å…³å†…å®¹ï¼‰
        if relevant_in_batch:
            sent = send_batch_to_feishu(relevant_in_batch)
            total_sent += sent
            total_relevant += len(relevant_in_batch)
            print(f"  æ‰¹æ¬¡ {batch_num}: å‘ç° {len(relevant_in_batch)} æ¡ç›¸å…³ï¼Œå·²å‘é€é£ä¹¦")
        
        # æ— è®ºæˆåŠŸå¤±è´¥ï¼Œéƒ½æ ‡è®°è¿™æ‰¹å†…å®¹ä¸ºå·²å¤„ç†
        for item in batch_items:
            item_id = item.get('id', item.get('link', ''))
            if item_id:
                processed_ids.add(item_id)
        
        # ç«‹å³ä¿å­˜å·²å¤„ç†è®°å½•ï¼ˆå¢é‡ä¿å­˜çš„å…³é”®ï¼‰
        save_processed_posts(processed_ids)
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ‰¹ï¼Œç­‰å¾…åå†å¤„ç†ä¸‹ä¸€æ‰¹
        if batch_num < total_batches:
            print(f"  ç­‰å¾… {REQUEST_DELAY} ç§’åå¤„ç†ä¸‹ä¸€æ‰¹...")
            time.sleep(REQUEST_DELAY)
    
    print("-" * 50)
    
    # å‘é€æ±‡æ€»é€šçŸ¥
    if total_relevant > 0:
        print("\nğŸ“¤ å‘é€æ±‡æ€»é€šçŸ¥...")
        send_summary_to_feishu({
            'total': len(new_items),
            'filtered': len(filtered_items),
            'relevant': total_relevant,
            'sent': total_sent,
            'posts': fetch_stats.get('post', 0),
            'comments': fetch_stats.get('comment', 0),
            'search': fetch_stats.get('search', 0),
            'relevant_posts': relevant_stats.get('post', 0),
            'relevant_comments': relevant_stats.get('comment', 0),
            'relevant_search': relevant_stats.get('search', 0),
        })
    
    # å®Œæˆ
    print("\n" + "=" * 60)
    print("âœ… è¿è¡Œå®Œæˆ!")
    print(f"   æ‰«æå†…å®¹: {len(new_items)} æ¡")
    print(f"     - å¸–å­: {fetch_stats.get('post', 0)}")
    print(f"     - è¯„è®º: {fetch_stats.get('comment', 0)}")
    print(f"     - æœç´¢: {fetch_stats.get('search', 0)}")
    print(f"   é¢„è¿‡æ»¤å: {len(filtered_items)} æ¡")
    print(f"   ç›¸å…³å†…å®¹: {total_relevant} æ¡")
    print(f"   æˆåŠŸæ¨é€: {total_sent} æ¡")
    print("=" * 60)


if __name__ == "__main__":
    main()
