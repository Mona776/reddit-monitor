"""
Reddit RSS æŠ“å–æ¨¡å—
é€šè¿‡RedditåŸç”ŸRSSè·å–å¸–å­ã€è¯„è®ºå’Œæœç´¢ç»“æœï¼Œæ— éœ€API Key
"""

import feedparser
import json
import os
import re
import time
import urllib.parse
from datetime import datetime
from typing import List, Dict, Optional
from bs4 import BeautifulSoup

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import (
    SUBREDDITS, POSTS_PER_SUBREDDIT, PROCESSED_POSTS_FILE, MAX_PROCESSED_POSTS,
    MONITOR_COMMENTS, COMMENTS_PER_SUBREDDIT,
    ENABLE_KEYWORD_SEARCH, SEARCH_KEYWORDS, SEARCH_RESULTS_PER_KEYWORD
)

# Reddit RSS è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…è¢«é™æµ
REQUEST_DELAY = 0.3


def clean_html(html_content: str) -> str:
    """æ¸…ç†HTMLå†…å®¹ï¼Œæå–çº¯æ–‡æœ¬"""
    if not html_content:
        return ""
    soup = BeautifulSoup(html_content, 'html.parser')
    # ç§»é™¤æ‰€æœ‰è„šæœ¬å’Œæ ·å¼
    for script in soup(["script", "style"]):
        script.decompose()
    text = soup.get_text(separator=' ', strip=True)
    # æ¸…ç†å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def get_item_id(entry: dict) -> str:
    """ä»RSS entryä¸­æå–å”¯ä¸€ID"""
    return entry.get('id', entry.get('link', ''))


def parse_feed_with_retry(url: str, max_retries: int = 3) -> Optional[feedparser.FeedParserDict]:
    """å¸¦é‡è¯•çš„RSSè§£æ"""
    for attempt in range(max_retries):
        try:
            feed = feedparser.parse(url, agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
            if not feed.bozo:
                return feed
            if attempt < max_retries - 1:
                time.sleep(REQUEST_DELAY)
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(REQUEST_DELAY)
    return None


def fetch_subreddit_posts(subreddit: str, limit: int = 10) -> List[Dict]:
    """
    è·å–æŒ‡å®šSubredditçš„æœ€æ–°å¸–å­
    """
    url = f"https://www.reddit.com/r/{subreddit}/new.rss?limit={limit}"
    
    feed = parse_feed_with_retry(url)
    if not feed:
        print(f"[è­¦å‘Š] è·å– r/{subreddit} å¸–å­å¤±è´¥")
        return []
    
    posts = []
    for entry in feed.entries[:limit]:
        content = ""
        if 'content' in entry:
            content = entry.content[0].value if entry.content else ""
        elif 'summary' in entry:
            content = entry.summary
        
        post = {
            'id': get_item_id(entry),
            'type': 'post',  # æ ‡è®°ç±»å‹
            'title': entry.get('title', ''),
            'content': clean_html(content),
            'link': entry.get('link', ''),
            'author': entry.get('author', 'unknown'),
            'subreddit': subreddit,
            'published': entry.get('published', ''),
        }
        posts.append(post)
    
    print(f"[å¸–å­] r/{subreddit}: è·å– {len(posts)} æ¡")
    time.sleep(REQUEST_DELAY)
    return posts


def fetch_subreddit_comments(subreddit: str, limit: int = 25) -> List[Dict]:
    """
    è·å–æŒ‡å®šSubredditçš„æœ€æ–°è¯„è®º
    """
    url = f"https://www.reddit.com/r/{subreddit}/comments.rss?limit={limit}"
    
    feed = parse_feed_with_retry(url)
    if not feed:
        print(f"[è­¦å‘Š] è·å– r/{subreddit} è¯„è®ºå¤±è´¥")
        return []
    
    comments = []
    for entry in feed.entries[:limit]:
        content = ""
        if 'content' in entry:
            content = entry.content[0].value if entry.content else ""
        elif 'summary' in entry:
            content = entry.summary
        
        # è¯„è®ºçš„æ ‡é¢˜é€šå¸¸åŒ…å«åŸå¸–ä¿¡æ¯
        title = entry.get('title', '')
        
        comment = {
            'id': get_item_id(entry),
            'type': 'comment',  # æ ‡è®°ç±»å‹
            'title': title,  # è¯„è®ºçš„ä¸Šä¸‹æ–‡æ ‡é¢˜
            'content': clean_html(content),
            'link': entry.get('link', ''),
            'author': entry.get('author', 'unknown'),
            'subreddit': subreddit,
            'published': entry.get('published', ''),
        }
        comments.append(comment)
    
    print(f"[è¯„è®º] r/{subreddit}: è·å– {len(comments)} æ¡")
    time.sleep(REQUEST_DELAY)
    return comments


def fetch_keyword_search(keyword: str, limit: int = 10) -> List[Dict]:
    """
    å…¨ç«™æœç´¢å…³é”®è¯
    """
    # URLç¼–ç å…³é”®è¯
    encoded_keyword = urllib.parse.quote(keyword)
    url = f"https://www.reddit.com/search.rss?q={encoded_keyword}&sort=new&limit={limit}"
    
    feed = parse_feed_with_retry(url)
    if not feed:
        print(f"[è­¦å‘Š] æœç´¢ '{keyword}' å¤±è´¥")
        return []
    
    results = []
    for entry in feed.entries[:limit]:
        content = ""
        if 'content' in entry:
            content = entry.content[0].value if entry.content else ""
        elif 'summary' in entry:
            content = entry.summary
        
        # å°è¯•ä»é“¾æ¥ä¸­æå–subreddit
        link = entry.get('link', '')
        subreddit_match = re.search(r'/r/([^/]+)/', link)
        subreddit = subreddit_match.group(1) if subreddit_match else 'unknown'
        
        result = {
            'id': get_item_id(entry),
            'type': 'search',  # æ ‡è®°ä¸ºæœç´¢ç»“æœ
            'title': entry.get('title', ''),
            'content': clean_html(content),
            'link': link,
            'author': entry.get('author', 'unknown'),
            'subreddit': subreddit,
            'published': entry.get('published', ''),
            'search_keyword': keyword,  # è®°å½•æœç´¢å…³é”®è¯
        }
        results.append(result)
    
    print(f"[æœç´¢] '{keyword}': è·å– {len(results)} æ¡")
    time.sleep(REQUEST_DELAY)
    return results


def load_processed_posts() -> set:
    """åŠ è½½å·²å¤„ç†çš„å¸–å­IDé›†åˆ"""
    try:
        if os.path.exists(PROCESSED_POSTS_FILE):
            with open(PROCESSED_POSTS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('processed_ids', []))
    except Exception as e:
        print(f"[è­¦å‘Š] åŠ è½½å·²å¤„ç†è®°å½•å¤±è´¥: {e}")
    return set()


def save_processed_posts(processed_ids: set):
    """ä¿å­˜å·²å¤„ç†çš„å¸–å­ID"""
    try:
        os.makedirs(os.path.dirname(PROCESSED_POSTS_FILE), exist_ok=True)
        
        ids_list = list(processed_ids)
        if len(ids_list) > MAX_PROCESSED_POSTS:
            ids_list = ids_list[-MAX_PROCESSED_POSTS:]
        
        data = {
            'processed_ids': ids_list,
            'last_updated': datetime.now().isoformat()
        }
        
        with open(PROCESSED_POSTS_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"[å­˜å‚¨] ä¿å­˜äº† {len(ids_list)} æ¡å·²å¤„ç†è®°å½•")
    except Exception as e:
        print(f"[é”™è¯¯] ä¿å­˜å·²å¤„ç†è®°å½•å¤±è´¥: {e}")


def fetch_all_new_posts() -> List[Dict]:
    """
    è·å–æ‰€æœ‰æ¥æºçš„æ–°å†…å®¹ï¼ˆå¸–å­ã€è¯„è®ºã€æœç´¢ç»“æœï¼‰
    
    Returns:
        æ–°å†…å®¹åˆ—è¡¨ï¼ˆå·²å»é‡ï¼‰
    """
    processed_ids = load_processed_posts()
    all_new_items = []
    
    stats = {'posts': 0, 'comments': 0, 'search': 0}
    
    # 1. è·å–Subredditå¸–å­
    print("\nğŸ“ è·å–å¸–å­...")
    print("-" * 40)
    for subreddit in SUBREDDITS:
        posts = fetch_subreddit_posts(subreddit, POSTS_PER_SUBREDDIT)
        for post in posts:
            if post['id'] not in processed_ids:
                all_new_items.append(post)
                processed_ids.add(post['id'])
                stats['posts'] += 1
    
    # 2. è·å–Subredditè¯„è®º
    if MONITOR_COMMENTS:
        print("\nğŸ’¬ è·å–è¯„è®º...")
        print("-" * 40)
        for subreddit in SUBREDDITS:
            comments = fetch_subreddit_comments(subreddit, COMMENTS_PER_SUBREDDIT)
            for comment in comments:
                if comment['id'] not in processed_ids:
                    all_new_items.append(comment)
                    processed_ids.add(comment['id'])
                    stats['comments'] += 1
    
    # 3. å…³é”®è¯æœç´¢
    if ENABLE_KEYWORD_SEARCH:
        print("\nğŸ” å…³é”®è¯æœç´¢...")
        print("-" * 40)
        for keyword in SEARCH_KEYWORDS:
            results = fetch_keyword_search(keyword, SEARCH_RESULTS_PER_KEYWORD)
            for result in results:
                if result['id'] not in processed_ids:
                    all_new_items.append(result)
                    processed_ids.add(result['id'])
                    stats['search'] += 1
    
    # ä¿å­˜æ›´æ–°åçš„å·²å¤„ç†è®°å½•
    save_processed_posts(processed_ids)
    
    # æ‰“å°ç»Ÿè®¡
    print(f"\n{'=' * 40}")
    print(f"[æ±‡æ€»] æ–°å†…å®¹ç»Ÿè®¡:")
    print(f"  - å¸–å­: {stats['posts']} æ¡")
    print(f"  - è¯„è®º: {stats['comments']} æ¡")
    print(f"  - æœç´¢: {stats['search']} æ¡")
    print(f"  - æ€»è®¡: {len(all_new_items)} æ¡")
    print(f"{'=' * 40}")
    
    return all_new_items


if __name__ == "__main__":
    # æµ‹è¯•è¿è¡Œ
    items = fetch_all_new_posts()
    for item in items[:5]:
        print(f"\n--- [{item['type']}] r/{item['subreddit']} ---")
        print(f"æ ‡é¢˜: {item['title'][:80]}...")
        print(f"é“¾æ¥: {item['link']}")
        if item.get('search_keyword'):
            print(f"å…³é”®è¯: {item['search_keyword']}")
